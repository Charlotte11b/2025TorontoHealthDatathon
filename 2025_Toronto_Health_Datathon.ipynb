{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMggN9IhERzuJxzSIyygdQL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Charlotte11b/2025TorontoHealthDatathon/blob/main/2025_Toronto_Health_Datathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sleep-disordered breathing (SDB), including obstructive sleep apnea (OSA), is a prevalent yet underdiagnosed condition with significant health implications. This condition affects an estimated 1 billion individuals worldwide, with moderate-to-severe OSA present in approximately 425 million people. Despite its significant health implications, including increased risk of cardiovascular disease, metabolic disorders, neurocognitive impairment, and reduced quality of life, OSA remains undiagnosed in up to 80% of affected individuals. Diagnosis is often delayed due to a complex and lengthy healthcare pathway, including waiting times to see a primary care physician, specialist referrals, and polysomnography (PSG) testing. In many healthcare systems, patients may wait several months to years for a formal diagnosis, particularly in regions with limited access to sleep specialists and diagnostic facilities.\n",
        "\n",
        "This study examines the predictive value of self-reported measures—such as demographics, comorbidities, and the STOP-BANG questionnaire—for objective sleep study metrics like the Respiratory Disturbance Index (RDI) and Apnea-Hypopnea Index (AHI). Self-reported measures offer a promising approach for screening and risk stratification, potentially reducing unnecessary polysomnography (PSG) referrals and improving diagnostic prioritization. Their predictive capability also enhances cost-effectiveness and accessibility, ensuring that high-risk individuals receive timely diagnostic testing while minimizing healthcare expenditures. Early identification of patients at risk for moderate-to-severe OSA facilitates early intervention and disease prevention, potentially reducing complications such as cardiovascular disease and cognitive decline.\n",
        "\n",
        "While the STOP-BANG questionnaire provides a validated risk score, machine learning (ML)-based models can improve predictive accuracy by learning complex patterns in the data, dynamically weighting features, and integrating additional predictors such as body mass index (BMI), neck circumference, comorbidities, and biomarkers. ML models also enable multimodal data integration, incorporating self-reported information with wearable device data (e.g., Fitbit, Apple Watch), electronic health records (EHRs), and imaging (e.g., upper airway MRI or CT scans). Unlike traditional categorical scoring, ML models provide continuous risk estimation, allowing for personalized thresholds and more precise risk stratification. Additionally, ML models offer feature importance analysis and explainability using methods like SHAP values and decision trees, improving clinical interpretability. These models can be adapted to specific populations, addressing demographic and physiological variations, and can be automated and scaled via clinical decision support systems (CDSS) and mobile health (mHealth) applications for real-time risk assessment.\n",
        "\n",
        "Leveraging machine learning in predictive modeling could significantly enhance clinical decision-making, optimize healthcare resource allocation, and improve access to timely OSA diagnosis and management."
      ],
      "metadata": {
        "id": "1I84jP__XYaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n",
        "!pip install shap\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip check\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import mutual_info_regression, RFE, SelectFromModel\n",
        "from sklearn.linear_model import LogisticRegression, LassoCV, LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, mean_squared_error, r2_score, mean_absolute_error\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.inspection import partial_dependence\n",
        "import statsmodels.api as sm"
      ],
      "metadata": {
        "id": "8wq-qaaCXaja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Loads the dataset from a CSV file and preprocesses it\n",
        "# defines target labels, encodes categorical variables, standardizes features, and splits into training/validation sets\n",
        "\n",
        "def load_data(csv_path, target_column, feature_columns):\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df[feature_columns + [target_column]].dropna()\n",
        "\n",
        "    # Encode categorical variables\n",
        "    for col in df.select_dtypes(include=['object', 'category']).columns:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "    # Split into train/validation sets\n",
        "    X = df[feature_columns]\n",
        "    y = df[target_column]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "    # Standardize numerical features\n",
        "    scaler = StandardScaler()\n",
        "    X_train[X_train.columns] = scaler.fit_transform(X_train)\n",
        "    X_val[X_val.columns] = scaler.transform(X_val)\n",
        "\n",
        "    return X_train, X_val, y_train, y_val"
      ],
      "metadata": {
        "id": "GnymCQ57XmFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the CSV path, target column, and feature columns\n",
        "csv_path = \"/home/jupyter/gcs/TCAIREM_SleepLabData.csv\"\n",
        "target_column = \"Slpahi\"\n",
        "feature_columns = [\n",
        "    'SBsnore', 'SBtired', 'SBObs', 'SBbp', 'SBneck',\n",
        "    'Sex', 'BMI', 'Height', 'Weight'\n",
        "]\n",
        "\n",
        "# Load the data\n",
        "X_train, X_val, y_train, y_val = load_data(csv_path, target_column, feature_columns)"
      ],
      "metadata": {
        "id": "WVs2j4ZiXmwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_selection(X_train, y_train):\n",
        "    # Mutual Information for Regression\n",
        "    mi_scores = mutual_info_regression(X_train, y_train)\n",
        "    mi_selected = X_train.columns[mi_scores > np.median(mi_scores)]\n",
        "\n",
        "    # Recursive Feature Elimination (RFE) with a Regression Model\n",
        "    model = LinearRegression()  # Use a regression model here\n",
        "    rfe = RFE(model, n_features_to_select=int(len(X_train.columns) / 2))\n",
        "    rfe.fit(X_train, y_train)\n",
        "    rfe_selected = X_train.columns[rfe.support_]\n",
        "\n",
        "    # LASSO Feature Selection\n",
        "    lasso = LassoCV(cv=5).fit(X_train, y_train)\n",
        "    lasso_selected = X_train.columns[np.abs(lasso.coef_) > 0]\n",
        "\n",
        "    # XGBoost Feature Selection\n",
        "    xgboost_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "    xgboost_model.fit(X_train, y_train)\n",
        "    xgboost_importance = xgboost_model.feature_importances_\n",
        "    xgboost_selected = X_train.columns[xgboost_importance > np.median(xgboost_importance)]\n",
        "\n",
        "    # SHAP Feature Selection\n",
        "    explainer = shap.Explainer(xgboost_model, X_train)\n",
        "    shap_values = explainer(X_train)\n",
        "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
        "    shap_selected = X_train.columns[shap_importance > np.median(shap_importance)]\n",
        "\n",
        "    # Union of selected features using OR\n",
        "    selected_features = set(mi_selected) | set(rfe_selected) | set(lasso_selected) | set(xgboost_selected) | set(shap_selected)\n",
        "\n",
        "    print(\"Selected Features from each method:\")\n",
        "    print(\"Mutual Information:\", list(mi_selected))\n",
        "    print(\"RFE:\", list(rfe_selected))\n",
        "    print(\"LASSO:\", list(lasso_selected))\n",
        "    print(\"XGBoost:\", list(xgboost_selected))\n",
        "    print(\"SHAP:\", list(shap_selected))\n",
        "    print(\"Final selected features:\", list(selected_features))\n",
        "\n",
        "    return list(selected_features)\n",
        "\n",
        "selected_features = feature_selection(X_train, y_train)"
      ],
      "metadata": {
        "id": "6YIseYdOXoKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_models(X_train, y_train, X_val, y_val):\n",
        "    # Use regression models\n",
        "    models = {\n",
        "        \"Linear Regression\": LinearRegression(),\n",
        "        \"Random Forest\": RandomForestRegressor(n_estimators=100),\n",
        "        \"XGBoost\": XGBRegressor(objective='reg:squarederror')\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "\n",
        "        # Use regression metrics\n",
        "        mse = mean_squared_error(y_val, y_pred)\n",
        "        mae = mean_absolute_error(y_val, y_pred)\n",
        "        r2 = r2_score(y_val, y_pred)\n",
        "\n",
        "        results[name] = {\"MSE\": mse, \"MAE\": mae, \"R2 Score\": r2}\n",
        "\n",
        "    # Find the best model based on R2 score\n",
        "    best_model_name = max(results, key=lambda k: results[k]['R2 Score'])\n",
        "    best_model = models[best_model_name]\n",
        "\n",
        "    print(\"Model Performance:\")\n",
        "    print(pd.DataFrame(results).T)\n",
        "    print(f\"Best Model: {best_model_name}\")\n",
        "\n",
        "    return best_model, best_model_name\n",
        "\n",
        "X_train, X_val = X_train[selected_features], X_val[selected_features]\n",
        "best_model, best_model_name = train_models(X_train, y_train, X_val, y_val)\n"
      ],
      "metadata": {
        "id": "Zbz8xpacXp76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert y (AHI score) into a categorical variable\n",
        "def convert_to_categories(y):\n",
        "    # Define categories based on the provided ranges\n",
        "    bins = [0, 5, 15, 30, float('inf')]  # Define bins for risk levels\n",
        "    labels = ['Normal', 'Low Risk', 'Moderate Risk', 'High Risk']\n",
        "    y_cat = pd.cut(y, bins=bins, labels=labels, right=False)\n",
        "    return y_cat"
      ],
      "metadata": {
        "id": "PJ4qCQH9XscT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "def train_and_evaluate_classifiers(X_train, y_train, X_val, y_val):\n",
        "    # Model Initialization\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, multi_class='ovr'),\n",
        "        'Random Forest': RandomForestClassifier(),\n",
        "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "    }\n",
        "\n",
        "    # Store performance metrics\n",
        "    performance_metrics = {}\n",
        "\n",
        "    # Train models and evaluate\n",
        "    for model_name, model in models.items():\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_val)\n",
        "\n",
        "        # Calculate metrics\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        prec = precision_score(y_val, y_pred, average='weighted')\n",
        "        sens = recall_score(y_val, y_pred, average='weighted')\n",
        "\n",
        "        # Confusion matrix to calculate specificity\n",
        "        cm = confusion_matrix(y_val, y_pred)\n",
        "        specificity = np.diag(cm) / cm.sum(axis=1)  # Specificity per class\n",
        "        avg_specificity = np.mean(specificity)\n",
        "\n",
        "        # Store results\n",
        "        performance_metrics[model_name] = {\n",
        "            'Accuracy': acc,\n",
        "            'Precision': prec,\n",
        "            'Sensitivity': sens,\n",
        "            'Specificity': avg_specificity\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame for easy comparison\n",
        "    performance_df = pd.DataFrame(performance_metrics).T\n",
        "    print(performance_df)\n",
        "\n",
        "    # Select best model based on accuracy\n",
        "    best_model_name = performance_df['Accuracy'].idxmax()\n",
        "    best_model = models[best_model_name]\n",
        "    print(f'\\nBest Model: {best_model_name}')\n",
        "\n",
        "    return performance_df, best_model_name\n"
      ],
      "metadata": {
        "id": "itX-r4B_Xt3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data: Convert y_train and y_val into categorical variables\n",
        "y_train_cat = convert_to_categories(y_train)\n",
        "y_val_cat = convert_to_categories(y_val)\n",
        "\n",
        "# Encode categorical labels as numeric labels\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train_cat)\n",
        "y_val_encoded = le.transform(y_val_cat)\n",
        "\n",
        "# Select the features based on feature selection (use the selected features)\n",
        "X_train, X_val = X_train[selected_features], X_val[selected_features]\n",
        "\n",
        "# Train and evaluate models\n",
        "performance_df, best_model_name = train_and_evaluate_classifiers(X_train, y_train_encoded, X_val, y_val_encoded)"
      ],
      "metadata": {
        "id": "h7p30h_vXwOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_nomogram(X_train, y_train, X_val, y_val):\n",
        "    # Fit a linear regression model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the validation set\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Mean squared error on validation set\n",
        "    mse = mean_squared_error(y_val, y_pred)\n",
        "    print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "    # Coefficients of the linear regression model\n",
        "    coefficients = model.coef_\n",
        "\n",
        "    # Create a nomogram-like plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot each feature's influence on the target\n",
        "    for i, coef in enumerate(coefficients):\n",
        "        # Make sure the feature name is correct\n",
        "        feature_name = X_train.columns[i]  # Get the feature name from the columns of X_train\n",
        "        plt.plot(X_train.iloc[:, i], X_train.iloc[:, i] * coef, label=f\"{feature_name} * Coef({coef:.2f})\")\n",
        "\n",
        "    # Adding plot aesthetics\n",
        "    plt.axhline(0, color='gray', lw=1)\n",
        "    plt.xlabel('Feature Values')\n",
        "    plt.ylabel('Influence on Target')\n",
        "    plt.title('Nomogram-like Visualization for Linear Regression Model')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage with your data:\n",
        "model = create_nomogram(X_train, y_train, X_val, y_val)\n"
      ],
      "metadata": {
        "id": "2QxP727yXyJc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}